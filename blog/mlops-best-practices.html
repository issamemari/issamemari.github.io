<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MLOps Best Practices for Production - Issa Memari</title>
    <link rel="stylesheet" href="../css/style.css">
    <link
        href="https://fonts.googleapis.com/css2?family=Crimson+Text:wght@400;600;700&family=Source+Code+Pro:wght@400;600&display=swap"
        rel="stylesheet">
</head>

<body>
    <div class="container">
        <header>
            <nav>
                <a href="../index.html" class="logo">IM</a>
                <ul>
                    <li><a href="../index.html#about">About</a></li>
                    <li><a href="../index.html#blog">Blog</a></li>
                    <li><a href="../index.html#contact">Contact</a></li>
                </ul>
            </nav>
        </header>

        <article class="blog-post">
            <div class="blog-header">
                <h1>MLOps Best Practices for Production</h1>
                <p class="meta">October 10, 2023 • 10 min read</p>
            </div>

            <div class="blog-post-content">
                <p>
                    Deploying machine learning models to production is just the beginning. The real challenge lies in
                    maintaining,
                    monitoring, and continuously improving these models at scale. This guide covers essential MLOps
                    practices that
                    ensure your ML systems remain reliable and performant in production environments.
                </p>

                <img src="../images/mlops-pipeline.png" alt="MLOps Pipeline Overview">

                <h2>1. Version Control Everything</h2>

                <p>
                    Reproducibility is the foundation of reliable ML systems. Version control should extend beyond code
                    to include:
                </p>

                <ul>
                    <li><strong>Data versioning:</strong> Track datasets and their transformations</li>
                    <li><strong>Model versioning:</strong> Store model artifacts with metadata</li>
                    <li><strong>Environment versioning:</strong> Pin dependencies and system configurations</li>
                </ul>

                <pre><code># Example: DVC for data versioning
dvc init
dvc add data/train_dataset.parquet
git add data/train_dataset.parquet.dvc .gitignore
git commit -m "Add training dataset v1.0"

# Track model with MLflow
import mlflow

with mlflow.start_run():
    mlflow.log_params({"learning_rate": 0.01, "batch_size": 32})
    mlflow.log_metrics({"accuracy": 0.95, "f1_score": 0.93})
    mlflow.pytorch.log_model(model, "model")</code></pre>

                <h2>2. Automated Testing for ML</h2>

                <p>
                    ML systems require specialized testing strategies beyond traditional software tests:
                </p>

                <h3>Data Validation Tests</h3>
                <pre><code>import great_expectations as ge

# Define expectations for input data
df = ge.read_csv("data/input.csv")
df.expect_column_values_to_be_between("age", min_value=0, max_value=120)
df.expect_column_values_to_not_be_null("user_id")
df.expect_column_values_to_be_in_set("country", ["US", "UK", "CA"])</code></pre>

                <h3>Model Performance Tests</h3>
                <pre><code>def test_model_performance():
    # Load test dataset
    X_test, y_test = load_test_data()
    
    # Get predictions
    predictions = model.predict(X_test)
    
    # Assert performance thresholds
    accuracy = accuracy_score(y_test, predictions)
    assert accuracy > 0.85, f"Model accuracy {accuracy} below threshold"
    
    # Check for class imbalance issues
    f1 = f1_score(y_test, predictions, average='weighted')
    assert f1 > 0.80, f"F1 score {f1} indicates poor performance"</code></pre>

                <h2>3. Continuous Training Pipeline</h2>

                <p>
                    Models decay over time due to data drift. Implement automated retraining pipelines:
                </p>

                <img src="../images/continuous-training.png" alt="Continuous Training Architecture">

                <pre><code># Airflow DAG for automated retraining
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'ml-team',
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'model_retraining',
    default_args=default_args,
    schedule_interval='@weekly',
    start_date=datetime(2023, 1, 1)
)

# Define tasks
validate_data = PythonOperator(
    task_id='validate_data',
    python_callable=validate_new_data,
    dag=dag
)

train_model = PythonOperator(
    task_id='train_model',
    python_callable=train_new_model,
    dag=dag
)

evaluate_model = PythonOperator(
    task_id='evaluate_model',
    python_callable=evaluate_performance,
    dag=dag
)

deploy_model = PythonOperator(
    task_id='deploy_model',
    python_callable=deploy_if_better,
    dag=dag
)

# Define dependencies
validate_data >> train_model >> evaluate_model >> deploy_model</code></pre>

                <h2>4. Model Monitoring and Observability</h2>

                <p>
                    Production models need comprehensive monitoring to detect issues early:
                </p>

                <h3>Key Metrics to Monitor</h3>
                <ul>
                    <li><strong>Prediction drift:</strong> Changes in prediction distribution</li>
                    <li><strong>Feature drift:</strong> Changes in input feature distributions</li>
                    <li><strong>Performance metrics:</strong> Accuracy, latency, throughput</li>
                    <li><strong>Business metrics:</strong> Revenue impact, user engagement</li>
                </ul>

                <pre><code># Monitoring with Prometheus and custom metrics
from prometheus_client import Histogram, Counter, Gauge

# Define metrics
prediction_latency = Histogram('model_prediction_duration_seconds', 
                             'Model prediction latency')
prediction_counter = Counter('model_predictions_total', 
                           'Total number of predictions')
drift_score = Gauge('model_drift_score', 
                   'Current drift score')

@prediction_latency.time()
def predict(features):
    # Your prediction logic
    result = model.predict(features)
    
    # Increment counter
    prediction_counter.inc()
    
    # Calculate and update drift score
    current_drift = calculate_drift(features)
    drift_score.set(current_drift)
    
    return result</code></pre>

                <h2>5. A/B Testing and Gradual Rollouts</h2>

                <p>
                    Never deploy new models to 100% of traffic immediately. Implement controlled rollouts:
                </p>

                <pre><code># Feature flag based model serving
import random
from feature_flags import get_flag_value

def get_model_prediction(user_id, features):
    # Determine which model to use
    if should_use_new_model(user_id):
        prediction = new_model.predict(features)
        log_prediction("new_model", user_id, prediction)
    else:
        prediction = old_model.predict(features)
        log_prediction("old_model", user_id, prediction)
    
    return prediction

def should_use_new_model(user_id):
    # Get rollout percentage from feature flag service
    rollout_percentage = get_flag_value("new_model_rollout", default=0)
    
    # Consistent hashing for stable assignment
    hash_value = hash(user_id) % 100
    return hash_value < rollout_percentage</code></pre>

                <h2>6. Model Governance and Compliance</h2>

                <p>
                    Establish clear governance practices for production ML:
                </p>

                <blockquote>
                    <p>
                        "In production ML, documentation and governance aren't optional - they're essential for
                        maintaining
                        trust and meeting compliance requirements."
                    </p>
                </blockquote>

                <h3>Model Cards</h3>
                <pre><code># model_card.yaml
model_details:
  name: "Customer Churn Predictor v2.1"
  version: "2.1.0"
  type: "Binary Classification"
  framework: "PyTorch 1.13"
  
intended_use:
  primary_uses: "Predict customer churn probability"
  primary_users: "Customer Success Team"
  out_of_scope: "Not for use with enterprise accounts"
  
training_data:
  dataset: "customer_data_2023Q1"
  size: "1.2M records"
  date_range: "2022-01-01 to 2023-03-31"
  
performance:
  accuracy: 0.89
  precision: 0.86
  recall: 0.91
  auc_roc: 0.93
  
ethical_considerations:
  biases: "Model shows lower accuracy for new customers (< 30 days)"
  fairness: "Tested for demographic parity across age groups"</code></pre>

                <h2>7. Infrastructure as Code</h2>

                <p>
                    Manage ML infrastructure declaratively for consistency and reproducibility:
                </p>

                <pre><code># terraform/ml_inference.tf
resource "kubernetes_deployment" "model_server" {
  metadata {
    name = "ml-model-server"
    labels = {
      app = "model-server"
    }
  }
  
  spec {
    replicas = var.model_server_replicas
    
    selector {
      match_labels = {
        app = "model-server"
      }
    }
    
    template {
      metadata {
        labels = {
          app = "model-server"
        }
      }
      
      spec {
        container {
          image = "${var.model_registry}/model-server:${var.model_version}"
          name  = "model-server"
          
          resources {
            limits = {
              cpu    = "2000m"
              memory = "4Gi"
              "nvidia.com/gpu" = "1"
            }
            requests = {
              cpu    = "1000m"
              memory = "2Gi"
            }
          }
          
          env {
            name  = "MODEL_PATH"
            value = "/models/production"
          }
        }
      }
    }
  }
}</code></pre>

                <h2>8. Disaster Recovery and Rollback</h2>

                <p>
                    Always have a plan B when things go wrong:
                </p>

                <ul>
                    <li>Maintain multiple model versions ready for instant rollback</li>
                    <li>Implement circuit breakers for model endpoints</li>
                    <li>Have fallback logic for model failures</li>
                    <li>Regular disaster recovery drills</li>
                </ul>

                <h2>MLOps Maturity Model</h2>

                <p>
                    Assess your MLOps maturity and plan improvements:
                </p>

                <ol>
                    <li><strong>Level 0:</strong> Manual ML - Scripts and notebooks</li>
                    <li><strong>Level 1:</strong> ML Pipeline Automation - Automated training</li>
                    <li><strong>Level 2:</strong> CI/CD for ML - Automated deployment</li>
                    <li><strong>Level 3:</strong> Automated ML Operations - Full MLOps</li>
                </ol>

                <h2>Conclusion</h2>

                <p>
                    MLOps is not just about deploying models - it's about building sustainable, reliable, and scalable
                    ML
                    systems. By implementing these practices, you'll be able to deliver value consistently while
                    maintaining
                    the agility to adapt to changing requirements.
                </p>

                <p>
                    Remember, MLOps is a journey, not a destination. Start with the basics, measure everything, and
                    continuously improve your processes based on what you learn in production.
                </p>
            </div>

            <div class="blog-nav">
                <a href="efficient-pytorch-training.html">← Previous: Efficient PyTorch Training</a>
                <a href="archive.html">View All Posts →</a>
            </div>
        </article>

        <footer>
            <p>&copy; 2026 Issa Memari. Built with ❤️ and vanilla HTML/CSS/JS.</p>
        </footer>
    </div>
</body>

</html>