<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Transformer Architecture - Issa Memari</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600&family=Newsreader:ital,wght@0,400;0,500;0,600;1,400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../css/modern-style.css">
</head>

<body>
    <header>
        <div class="container">
            <div class="header-content">
                <a href="../index.html" class="logo">issa<span>.</span>memari</a>
                <nav>
                    <a href="../index.html#about">About</a>
                    <a href="../index.html#blog">Writing</a>
                    <a href="https://github.com/issamemari" target="_blank">GitHub</a>
                </nav>
            </div>
        </div>
    </header>

    <div class="container">
        <div class="article-header">
            <div class="article-meta">
                <span class="tag">Deep Learning</span>
                <span>December 15, 2023</span>
                <span>8 min read</span>
            </div>
            <h1>Understanding Transformer Architecture</h1>
            <p class="subtitle">A deep dive into the self-attention mechanism, multi-head attention, and why transformers have revolutionized natural language processing.</p>
        </div>

        <article>
            <p>
                The Transformer architecture has revolutionized the field of natural language processing and, more recently, 
                computer vision. Introduced in the groundbreaking paper "Attention is All You Need" by Vaswani et al., 
                transformers have become the foundation for models like BERT, GPT, and many others.
            </p>

            <img src="../images/transformer-architecture.png" alt="Transformer Architecture Diagram">

            <h2>The Core Innovation: Self-Attention</h2>
            
            <p>
                At the heart of the transformer architecture lies the self-attention mechanism. Unlike RNNs that process 
                sequences sequentially, transformers can process all positions simultaneously, making them highly parallelizable 
                and efficient.
            </p>

            <p>
                The self-attention mechanism computes three vectors for each input token:
            </p>

            <ul>
                <li><strong>Query (Q):</strong> What information am I looking for?</li>
                <li><strong>Key (K):</strong> What information do I have?</li>
                <li><strong>Value (V):</strong> What is the actual information content?</li>
            </ul>

            <h3>The Attention Formula</h3>

            <p>The attention scores are computed using the following formula:</p>

            <pre data-lang="math"><code>Attention(Q, K, V) = softmax(QK^T / √d_k)V</code></pre>

            <p>
                Where d_k is the dimension of the key vectors. The scaling factor 1/√d_k prevents the dot products from 
                growing too large, which would push the softmax function into regions with extremely small gradients.
            </p>

            <h2>Multi-Head Attention</h2>

            <p>
                Instead of performing a single attention function, transformers use multi-head attention. This allows the 
                model to jointly attend to information from different representation subspaces at different positions.
            </p>

            <div class="callout education">
                <p><strong>Multi-head attention</strong> allows the model to capture different types of relationships. 
                For example, one head might focus on syntactic relationships while another captures semantic dependencies.</p>
            </div>

            <img src="../images/multi-head-attention.png" alt="Multi-Head Attention Visualization">

            <p>
                Each attention head learns different relationships between tokens, capturing various aspects of the input 
                sequence. The outputs from all heads are concatenated and linearly transformed to produce the final output.
            </p>

            <h2>Positional Encoding</h2>

            <p>
                Since transformers don't have any inherent notion of position or order, we need to inject positional 
                information into the model. This is done through positional encodings, which are added to the input embeddings.
            </p>

            <pre data-lang="python"><code>PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</code></pre>

            <p>
                These sinusoidal functions were chosen because they allow the model to easily learn to attend by relative 
                positions, since for any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos).
            </p>

            <h2>The Complete Architecture</h2>

            <p>
                A transformer consists of an encoder and decoder, each made up of multiple layers. Each encoder layer contains:
            </p>

            <ol>
                <li>Multi-head self-attention mechanism</li>
                <li>Position-wise feed-forward network</li>
                <li>Residual connections and layer normalization</li>
            </ol>

            <p>
                The decoder has a similar structure but includes an additional masked multi-head attention layer to prevent 
                positions from attending to subsequent positions during training.
            </p>

            <div class="architecture-diagram"><code>┌─────────────────────────────────────────────────────────────────────┐
│                           Transformer Architecture                    │
│                                                                       │
│  ┌─────────────────┐                      ┌─────────────────┐       │
│  │    Encoder      │                      │     Decoder     │       │
│  │                 │                      │                 │       │
│  │  ┌───────────┐  │                      │  ┌───────────┐  │       │
│  │  │ Multi-    │  │                      │  │ Masked    │  │       │
│  │  │ Head      │  │                      │  │ Multi-    │  │       │
│  │  │ Attention │  │                      │  │ Head      │  │       │
│  │  └───────────┘  │                      │  │ Attention │  │       │
│  │        ↓        │                      │  └───────────┘  │       │
│  │  ┌───────────┐  │                      │        ↓        │       │
│  │  │ Add &     │  │                      │  ┌───────────┐  │       │
│  │  │ Norm      │  │                      │  │ Multi-    │  │       │
│  │  └───────────┘  │                      │  │ Head      │  │       │
│  │        ↓        │                      │  │ Attention │  │       │
│  │  ┌───────────┐  │                      │  └───────────┘  │       │
│  │  │ Feed      │  │                      │        ↓        │       │
│  │  │ Forward   │  │                      │  ┌───────────┐  │       │
│  │  └───────────┘  │                      │  │ Feed      │  │       │
│  │        ↓        │                      │  │ Forward   │  │       │
│  │  ┌───────────┐  │                      │  └───────────┘  │       │
│  │  │ Add &     │  │                      │        ↓        │       │
│  │  │ Norm      │  │                      │  ┌───────────┐  │       │
│  │  └───────────┘  │                      │  │ Linear &  │  │       │
│  │                 │                      │  │ Softmax   │  │       │
│  └─────────────────┘                      │  └───────────┘  │       │
│                                           └─────────────────┘       │
└─────────────────────────────────────────────────────────────────────┘</code></div>

            <h2>Why Transformers Work So Well</h2>

            <p>Several factors contribute to the success of transformers:</p>

            <blockquote>
                The transformer's ability to capture long-range dependencies without the limitations of sequential 
                processing makes it particularly effective for understanding context in natural language.
            </blockquote>

            <ul>
                <li><strong>Parallelization:</strong> Unlike RNNs, all positions can be processed simultaneously</li>
                <li><strong>Long-range dependencies:</strong> Self-attention can directly connect distant positions</li>
                <li><strong>Interpretability:</strong> Attention weights provide insights into what the model is "looking at"</li>
                <li><strong>Flexibility:</strong> The architecture can be adapted for various tasks and modalities</li>
            </ul>

            <h2>Practical Implementation Tips</h2>

            <p>When implementing transformers in PyTorch, here are some key considerations:</p>

            <pre data-lang="python"><code>import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # Linear transformations and split into heads
        Q = self.w_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k).float())
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
            
        attention_weights = torch.softmax(scores, dim=-1)
        context = torch.matmul(attention_weights, V)
        
        # Concatenate heads
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        output = self.w_o(context)
        
        return output</code></pre>

            <div class="callout insight">
                <p>When implementing transformers, pay special attention to the dimensionality of your tensors. 
                The reshape and transpose operations are critical for proper multi-head processing.</p>
            </div>

            <h2>Conclusion</h2>

            <p>
                The transformer architecture represents a paradigm shift in how we approach sequence modeling. Its elegant 
                design and impressive performance have made it the go-to architecture for a wide range of tasks beyond NLP, 
                including computer vision (Vision Transformer), protein folding (AlphaFold), and more.
            </p>

            <p>
                As we continue to push the boundaries of what's possible with transformers, understanding their fundamental 
                principles becomes increasingly important for any ML engineer working with modern deep learning systems.
            </p>
        </article>

        <div class="blog-nav">
            <a href="../index.html">← Back to Home</a>
            <a href="efficient-pytorch-training.html">Next: Efficient PyTorch Training →</a>
        </div>
    </div>

    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-links">
                    <a href="https://github.com/issamemari">GitHub</a>
                    <a href="https://www.linkedin.com/in/issa-memari/">LinkedIn</a>
                    <a href="mailto:issa@memari.me">Email</a>
                </div>
                <div class="footer-copy">© 2026 Issa Memari</div>
            </div>
        </div>
    </footer>
</body>

</html>