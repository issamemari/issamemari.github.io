<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Implementing Radix Select for the GPU: A Worklog | Issa Memari</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600&family=Newsreader:ital,wght@0,400;0,500;0,600;1,400&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="../css/modern-style.css">
</head>

<body>
    <header>
        <div class="container">
            <div class="header-content">
                <a href="../index.html" class="logo">issa<span>.</span>memari</a>
                <nav>
                    <a href="../index.html#about">About</a>
                    <a href="../index.html#blog">Writing</a>
                    <a href="https://github.com/issamemari" target="_blank">GitHub</a>
                </nav>
            </div>
        </div>
    </header>

    <div class="container">
        <div class="article-header">
            <div class="article-meta">
                <span class="tag">GPU</span>
                <span>January 2026</span>
            </div>
            <h1>Implementing Radix Select for the GPU: A Worklog</h1>
        </div>

        <article>
            <p>In this post, I'll implement a radix-based selection algorithm for GPUs from scratch. My goal is to
                build a TopK kernel that's actually competitive with the state of the art—which, as I discovered the
                hard way, is a higher bar than I expected.</p>

            <h2>The Humbling</h2>

            <p>I started this project thinking I had a decent TopK implementation. I was combining ideas from <a
                    href="https://arxiv.org/abs/2104.13519">Dr. TopK</a><sup>1</sup>
                with a simple selection approach: loop <code>i = 0</code> to <code>K</code>, find the maximum in
                <code>array[i:]</code>, swap it with <code>array[i]</code>. After some optimizations—parallel
                reductions, warp-level operations, block-level coordination—I thought I had myself a pretty good kernel.
            </p>

            <p class="footnote"><sup>1</sup> Dr. TopK splits the array into ranges, finds delegates (maximum per range), keeps the TopK among the delegates, removes ranges whose delegate isn't in the TopK, then concatenates and runs TopK on what remains.</p>

            <p>Then I benchmarked against Thrust.</p>

            <pre><code>./dr_topk_parallel_topk_unroll_warp_parallel_block_reduction_param_optimized 100000000 1000
Running dr_topk with N=100000000, K=1000
=== Algorithm Performance Report ===
N = 100000000, K = 1000

Step 1 - Data preparation & H2D copy: 5554.255 ms
Step 2 - Find delegates (RW=4096, threads=24415): 31.818 ms
Step 3 - Prepare delegate indices & H2D copy: 0.231 ms
Step 4 - Find top-K from delegates: 19.117 ms
Step 5 - Concatenate top-K ranges: 0.246 ms
Step 6 - Final top-K on concatenated ranges: 282.780 ms
Step 7 - D2H copy: 0.082 ms</code></pre>

            <p>The total kernel time is around 334ms. Now compare to Thrust just... sorting the entire array:</p>

            <pre><code>./thrust_topk 100000000 1000
=== Thrust Sort TopK ===
N = 100000000, K = 1000

Step 1 - Data preparation & H2D copy: 5380.762 ms
Step 2 - Thrust sort (descending): 39.574 ms
Step 3 - D2H copy: 0.043 ms</code></pre>

            <p><strong>39ms.</strong> Thrust can <em>sort 100 million elements</em> faster than my kernel can find the
                top 1000. And my kernel should be faster—I'm doing less work! I only need to find K items, not establish a
                total ordering.</p>

            <p>This was deflating. But also clarifying: my baseline should be sorting with Thrust and taking the first
                K. If I want to beat that, I need to use the same underlying technique Thrust uses. I dug into the
                <a href="https://github.com/nvidia/thrust">Thrust source code</a> and found the answer: radix sort.
            </p>

            <h2>A Quick Primer on Radix Sort</h2>

            <p>Radix sort is one of those algorithms that seems almost too simple to work, and yet it's the foundation
                of the fastest GPU sorting implementations.<sup>2</sup></p>

            <p class="footnote"><sup>2</sup> It's a non-comparison sort, which lets it break the <code>O(n log n)</code> lower bound for comparison-based sorts. The complexity is <code>O(nw)</code> where <code>w</code> is the number of digits/bits.</p>

            <p>The basic idea: repeatedly bucket elements by their digits, starting from the least significant. Here's a
                worked example with decimal numbers:</p>

            <pre><code>array: [23, 66, 12, 539, 12, 32, 61]</code></pre>

            <p><strong>Pass 1: Bucket by ones digit</strong></p>

            <pre><code>bucket 0:
bucket 1: 61
bucket 2: 12, 12, 32
bucket 3: 23
bucket 6: 66
bucket 9: 539</code></pre>

            <p>Unload in bucket order: <code>[61, 12, 12, 32, 23, 66, 539]</code></p>

            <p><strong>Pass 2: Bucket by tens digit</strong></p>

            <pre><code>bucket 1: 12, 12
bucket 2: 23
bucket 3: 32, 539
bucket 6: 61, 66</code></pre>

            <p>Unload: <code>[12, 12, 23, 32, 539, 61, 66]</code></p>

            <p><strong>Pass 3: Bucket by hundreds digit</strong></p>

            <pre><code>bucket 0: 12, 12, 23, 32, 61, 66
bucket 5: 539</code></pre>

            <p>Unload: <code>[12, 12, 23, 32, 61, 66, 539]</code> ✓</p>

            <p>That's it. The array is sorted. The magic is that because we process least-significant digits first and the bucketing is stable (elements maintain their relative order within buckets), each pass refines the previous without destroying it.</p>

            <p>In practice, GPU implementations work on binary and process multiple bits at once—typically 4-8 bits per
                pass, giving 16-256 buckets. But the principle is identical.</p>

            <h2>From Sorting to Selection</h2>

            <p>Here's where it gets interesting. We don't need to sort. We just need the top K.</p>

            <p>Consider the same array, but now we want the top 3 elements. Instead of processing least-significant bits first, let's start from the <em>most</em> significant digit:</p>

            <pre><code>array: [23, 66, 12, 539, 12, 32, 61]
K = 3</code></pre>

            <p><strong>Pass 1: Bucket by hundreds digit</strong></p>

            <pre><code>bucket 0: 23, 66, 12, 12, 32, 61  (6 elements)
bucket 5: 539                     (1 element)</code></pre>

            <p>Stop and think. The bucket 5 contains 1 element. Since we want the top 3, and bucket 5 is the highest bucket, all of its elements are <em>guaranteed</em> to be in the top K. We've found one answer: <strong>539</strong>.</p>

            <p>Now we need 2 more elements from bucket 0.</p>

            <p><strong>Pass 2: Bucket the remaining elements by tens digit</strong></p>

            <pre><code>bucket 1: 12, 12
bucket 2: 23
bucket 3: 32
bucket 6: 66, 61</code></pre>

            <p>Again, work from the top. Bucket 6 has 2 elements. Combined with the 1 we already found, that's exactly 3. Done.</p>

            <p><strong>Top 3: [539, 66, 61]</strong> ✓</p>

            <p>The key insight: at each digit position, we can immediately identify elements that <em>must</em> be in the top K (buckets at the high end whose cumulative count from the top doesn't exceed K) and elements that <em>can't</em> be (buckets at the low end whose cumulative count from the bottom exceeds N-K). We only recurse into the <em>pivot bucket</em>—the one that straddles the boundary.</p>

            <h2>The Algorithm, Formally</h2>

            <p>Here's a Python implementation that captures the recursive structure:</p>

            <pre><code>def radix_select(array, k, digit_rank) -> list:
    if digit_rank < 0:
        return list(array)[:k]

    if len(array) == 0:
        return []

    if len(array) <= k:
        return list(array)

    # Bucket by current digit
    buckets = [[] for _ in range(10)]
    for number in array:
        digit = (number // (10**digit_rank)) % 10
        buckets[digit].append(number)

    # Scan from highest bucket downward
    found = []
    for i in range(len(buckets))[::-1]:
        if len(buckets[i]) == 0:
            continue

        if len(found) + len(buckets[i]) <= k:
            # This entire bucket is in top-K
            found.extend(buckets[i])
            if len(found) == k:
                return found
        else:
            # This bucket straddles the boundary—recurse
            needed = k - len(found)
            return found + radix_select(buckets[i], needed, digit_rank - 1)

    return found</code></pre>

            <h2>From Python to CUDA: The Practical Challenges</h2>

            <p>Before we can implement this on a GPU, we need to address some impedance mismatches between the Python sketch and CUDA C.</p>

            <h3>Dynamic Allocation is the Enemy</h3>

            <p>The Python implementation casually creates new lists for each bucket at each recursion level. This is
                fine for a 20-line prototype, but on a GPU, dynamic memory allocation is expensive—and doing it from
                within a kernel is even worse.<sup>3</sup></p>

            <p class="footnote"><sup>3</sup> You <em>can</em> use <code>malloc</code> from device code (since compute capability 2.x), but it's backed by a fixed-size heap and has terrible performance characteristics for fine-grained allocations.</p>

            <p>The fix is a classic trick: <strong>pre-allocate once, partition logically</strong>. We allocate a single
                output array the same size as the input, then use index arithmetic to carve it into buckets.</p>

            <p>The implementation requires two passes:</p>
            <ol>
                <li><strong>Histogram pass</strong>: Count how many elements fall into each bucket</li>
                <li><strong>Scatter pass</strong>: Compute prefix sums to get bucket offsets, then write each element to
                    its destination</li>
            </ol>

            <pre><code>Input:  [23, 66, 12, 539, 12, 32, 61]

Pass 1 - Count by hundreds digit:
  bucket 0: 6 elements
  bucket 5: 1 element

Pass 2 - Compute offsets via prefix sum:
  bucket 0 starts at index 0
  bucket 5 starts at index 6

Pass 3 - Scatter to output:
  Output: [23, 66, 12, 12, 32, 61, 539]
           └─── bucket 0 ───┘   └─ bucket 5</code></pre>

            <p>Each thread reads one element, determines its bucket, atomically increments a per-bucket counter to
                claim a slot, and writes to that slot. The atomic increment is the only synchronization needed.<sup>4</sup></p>

            <p class="footnote"><sup>4</sup> This is still a simplification. Real implementations use per-block histograms to reduce contention, then combine them. We'll get there.</p>

            <h3>The Negative Number Problem</h3>

            <p>There's a more subtle issue lurking in our algorithm. Let's trace through radix select with some negative numbers:</p>

            <pre><code>array: [23, -66, 12, -539, 12, 32, -61]
K = 3 (we want the three largest: 32, 23, 12)</code></pre>

            <p>Our algorithm buckets by the most significant digit first. For the hundreds place:</p>

            <pre><code>digit 0: 23, 12, 12, 32   (4 elements)
digit 5: -539             (1 element)</code></pre>

            <p>Wait—where do -66 and -61 go? They don't have a hundreds digit in the same sense. And -539: is its hundreds digit 5, or is it somehow "negative 5"?</p>

            <p>Let's try a simpler example. Consider sorting these numbers in descending order:</p>

            <pre><code>array: [5, -3, 2, -1, 4]</code></pre>

            <p>The correct descending order is: <strong>[5, 4, 2, -1, -3]</strong></p>

            <p>If we bucket by the ones digit (ignoring the sign for now):</p>

            <pre><code>digit 1: -1
digit 2: 2
digit 3: -3
digit 4: 4
digit 5: 5</code></pre>

            <p>Scanning from highest bucket down for K=2, we'd pick 5 and 4. That happens to be correct! But we got lucky—let's try K=3:</p>

            <p>Scanning down: digit 5 gives us 5, digit 4 gives us 4, digit 3 gives us... <strong>-3</strong>.</p>

            <p>We'd return <strong>[5, 4, -3]</strong>, but the correct answer is <strong>[5, 4, 2]</strong>.</p>

            <p>The problem is fundamental: our algorithm assumes that higher digits mean larger values. But -3 isn't larger than 2 just because 3 > 2. The sign completely changes the ordering.</p>

            <h4>What We Need</h4>

            <p>For radix select to work, we need a representation where the digit-by-digit comparison matches numeric ordering. Specifically:</p>

            <ol>
                <li>All positive numbers should sort higher than all negative numbers</li>
                <li>Among positives, larger magnitude = larger value (this already works)</li>
                <li>Among negatives, larger magnitude = <em>smaller</em> value (-1 > -100)</li>
            </ol>

            <p>Decimal digits can't give us this directly. We need to transform the numbers into a representation where "lexicographically larger" equals "numerically larger."</p>

            <p>This is where we'll need to dive into how computers actually represent negative numbers—using a system called two's complement. The good news: there's a simple bit-manipulation trick that makes everything work. But first, let's understand the representation.</p>

            <h4>A Primer on Two's Complement</h4>

            <p>Let's work with 8-bit integers first—the principles scale to 32 and 64 bits.</p>

            <p>An <strong>unsigned 8-bit integer</strong> (<code>uint8</code>) uses all 8 bits to represent magnitude. The range is 0 to 255:</p>

            <pre><code>0   = 0b00000000
1   = 0b00000001
127 = 0b01111111
128 = 0b10000000
255 = 0b11111111</code></pre>

            <p>Simple enough: the bit pattern <em>is</em> the number in binary. Larger values mean larger bit patterns, and vice versa. Radix sort on unsigned integers just works.</p>

            <p>For <strong>signed 8-bit integers</strong> (<code>int8</code>), we need to represent negative numbers somehow. The nearly universal solution is <strong>two's complement</strong>. The range is -128 to 127:</p>

            <pre><code> 127 = 0b01111111
 126 = 0b01111110
   1 = 0b00000001
   0 = 0b00000000
  -1 = 0b11111111
  -2 = 0b11111110
-127 = 0b10000001
-128 = 0b10000000</code></pre>

            <p>Notice the pattern:</p>
            <ul>
                <li><strong>Positive numbers</strong> (0 to 127): MSB is 0, remaining bits are the magnitude. Same as unsigned.</li>
                <li><strong>Negative numbers</strong> (-1 to -128): MSB is 1. To get the magnitude, flip all bits and add 1.<sup>5</sup></li>
            </ul>

            <p class="footnote"><sup>5</sup> This is why it's called <em>two's complement</em>—the negative of a number is computed by taking the bitwise complement and adding one. Equivalently: <code>-x = ~x + 1</code>.</p>

            <p>Let's verify with -1:</p>

            <pre><code>-1 in two's complement: 0b11111111
Flip all bits:          0b00000000
Add 1:                  0b00000001 = 1 ✓</code></pre>

            <p>And -128:</p>

            <pre><code>-128 in two's complement: 0b10000000
Flip all bits:            0b01111111
Add 1:                    0b10000000 = 128 ✓</code></pre>

            <p>Two's complement has a beautiful property: addition <em>just works</em> without special-casing the sign. The hardware doesn't need separate circuits for signed vs unsigned addition.<sup>6</sup></p>

            <p class="footnote"><sup>6</sup> This is why two's complement won the representation wars. One's complement and sign-magnitude both require special handling for arithmetic operations.</p>

            <h4>Why This Breaks Radix Select</h4>

            <p>Back to our example, now with 8-bit two's complement:</p>

            <pre><code>array: [5, -3, 2, -1, 4]
K = 2 (we want the two largest: 5 and 4)</code></pre>

            <p>The bit representations:</p>

            <pre><code>  5 = 0b00000101
  4 = 0b00000100
  2 = 0b00000010
 -1 = 0b11111111
 -3 = 0b11111101</code></pre>

            <p>Our algorithm buckets by most significant bits first. With a 2-bit radix (4 buckets), looking at bits 7-6:</p>

            <pre><code>Bucket 0b00 (0): 5, 4, 2  (all positives)
Bucket 0b01 (1): (empty)
Bucket 0b10 (2): (empty)
Bucket 0b11 (3): -1, -3   (all negatives)</code></pre>

            <p>The algorithm scans from the highest bucket down. Bucket 3 has 2 elements—exactly K! So it returns <strong>[-1, -3]</strong> as the top 2.</p>

            <p>But the actual top 2 are <strong>[5, 4]</strong>. We're completely wrong.</p>

            <p>The root cause: when interpreted as <em>unsigned</em> integers, the bit patterns of negative numbers are larger than those of positive numbers:</p>

            <pre><code>Signed → Unsigned interpretation
  5 (0b00000101) →   5
  4 (0b00000100) →   4
  2 (0b00000010) →   2
 -1 (0b11111111) → 255
 -3 (0b11111101) → 253</code></pre>

            <p>Radix sort/select operates on bit patterns, not semantic values. It sees 255 and 253 as the largest numbers.</p>

            <p>To make this concrete, here's how numeric ordering compares to bit-pattern ordering:</p>

            <pre><code>Numeric order (signed):    -128 < -1 < 0 < 1 < 127
Bit-pattern order (uint):   0 < 1 < 127 < 128 < 255
                            ↑   ↑    ↑     ↑     ↑
Actual signed values:       0   1  127  -128   -1</code></pre>

            <p>The positive numbers are fine—their relative order is preserved. But negatives come <em>after</em> all positives in bit-pattern order, and they're internally reversed: -1 (0xFF) appears larger than -128 (0x80).</p>

            <h4>The Fix: Flip the Sign Bit</h4>

            <p>We need a transformation that makes bit-pattern ordering match numeric ordering. The solution is surprisingly simple: <strong>flip the most significant bit</strong>.</p>

            <pre><code>Original → Flip MSB
  5 (0b00000101) → 0b10000101 (133)
  4 (0b00000100) → 0b10000100 (132)
  2 (0b00000010) → 0b10000010 (130)
 -1 (0b11111111) → 0b01111111 (127)
 -3 (0b11111101) → 0b01111101 (125)</code></pre>

            <p>Now let's check the ordering of the transformed values:</p>

            <pre><code>Transformed: 125 < 127 < 130 < 132 < 133
Original:     -3   -1     2     4     5  ✓</code></pre>

            <p>It works. The transformation preserves numeric ordering while giving us unsigned bit patterns we can radix-sort. Here's why:</p>

            <ol>
                <li><strong>Positive numbers</strong> originally have MSB=0. Flipping makes MSB=1, so they all move to the upper half of the unsigned range (128-255 for uint8).</li>
                <li><strong>Negative numbers</strong> originally have MSB=1. Flipping makes MSB=0, so they all move to the lower half (0-127 for uint8).</li>
                <li><strong>Within positive numbers</strong>, the relative ordering is unchanged—we only flipped the top bit, which was the same (0) for all of them.</li>
                <li><strong>Within negative numbers</strong>, the relative ordering is also unchanged—same logic.</li>
                <li><strong>Across the boundary</strong>, all (transformed) positives are now greater than all (transformed) negatives, which matches numeric ordering.</li>
            </ol>

            <p>For 32-bit integers:</p>

            <pre><code>// Transform signed int to radix-sortable unsigned
__device__ __host__ unsigned int to_sortable(int x) {
    return (unsigned int)x ^ 0x80000000u;  // flip MSB
}

// Transform back
__device__ __host__ int from_sortable(unsigned int x) {
    return (int)(x ^ 0x80000000u);  // flip MSB again (self-inverse)
}</code></pre>

            <p>The <code>0x80000000u</code> constant is just a 32-bit value with only the MSB set. XOR with this flips exactly that bit.</p>

            <p>We apply <code>to_sortable()</code> before radix select, run the algorithm on unsigned values, then apply <code>from_sortable()</code> to the results. The transformation is its own inverse, so we use the same XOR operation both ways.</p>

            <p>This same approach extends to floating-point numbers, though the transformation is slightly more involved—IEEE 754 floats use sign-magnitude representation rather than two's complement, so we need to handle positive and negative floats differently. We'll cover that when we need it.<sup>7</sup></p>

            <p class="footnote"><sup>7</sup> Quick preview: for positive floats, flip the sign bit (like integers). For negative floats, flip <em>all</em> bits. This handles the fact that negative floats are "backwards"—(-1.0) has a larger bit pattern than (-2.0) despite being numerically larger.</p>
        </article>
    </div>

    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-links">
                    <a href="https://github.com/issamemari">GitHub</a>
                    <a href="https://www.linkedin.com/in/issa-memari/">LinkedIn</a>
                    <a href="mailto:issa@memari.me">Email</a>
                </div>
                <div class="footer-copy">© 2026 Issa Memari</div>
            </div>
        </div>
    </footer>
</body>

</html>
