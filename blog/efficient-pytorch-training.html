<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Efficient PyTorch Training Tips - Issa Memari</title>
    <link rel="stylesheet" href="../css/style.css">
    <link
        href="https://fonts.googleapis.com/css2?family=Crimson+Text:wght@400;600;700&family=Source+Code+Pro:wght@400;600&display=swap"
        rel="stylesheet">
</head>

<body>
    <div class="container">
        <header>
            <nav>
                <a href="../index.html" class="logo">IM</a>
                <ul>
                    <li><a href="../index.html#about">About</a></li>
                    <li><a href="../index.html#blog">Blog</a></li>
                    <li><a href="../index.html#contact">Contact</a></li>
                </ul>
            </nav>
        </header>

        <article class="blog-post">
            <div class="blog-header">
                <h1>Efficient PyTorch Training Tips</h1>
                <p class="meta">November 28, 2023 • 6 min read</p>
            </div>

            <div class="blog-post-content">
                <p>
                    Training deep learning models efficiently is crucial for both research productivity and production
                    deployment.
                    In this post, I'll share practical tips and techniques to speed up your PyTorch training pipelines
                    while
                    reducing memory usage.
                </p>

                <img src="../images/pytorch-training.png" alt="PyTorch Training Pipeline">

                <h2>1. Mixed Precision Training</h2>

                <p>
                    Mixed precision training uses both 16-bit and 32-bit floating-point types to reduce memory usage and
                    speed up
                    training on modern GPUs with Tensor Core support.
                </p>

                <pre><code>from torch.cuda.amp import autocast, GradScaler

# Initialize gradient scaler
scaler = GradScaler()

for epoch in range(num_epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        
        # Automatic mixed precision
        with autocast():
            output = model(data)
            loss = criterion(output, target)
        
        # Scale loss and backward
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()</code></pre>

                <p>
                    Using mixed precision can provide up to 3x speedup on V100/A100 GPUs while maintaining model
                    accuracy.
                    The key is that most operations can be performed in FP16, while critical operations like loss
                    scaling
                    remain in FP32.
                </p>

                <h2>2. Gradient Accumulation</h2>

                <p>
                    When your batch size is limited by GPU memory, gradient accumulation allows you to simulate larger
                    batch
                    sizes by accumulating gradients over multiple forward passes.
                </p>

                <pre><code>accumulation_steps = 4
for epoch in range(num_epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        with autocast():
            output = model(data)
            loss = criterion(output, target) / accumulation_steps
        
        scaler.scale(loss).backward()
        
        if (batch_idx + 1) % accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()</code></pre>

                <h2>3. Efficient Data Loading</h2>

                <p>
                    Data loading can often be a bottleneck in training pipelines. Here are key optimizations:
                </p>

                <ul>
                    <li><strong>Multiple workers:</strong> Use num_workers > 0 in DataLoader</li>
                    <li><strong>Pin memory:</strong> Set pin_memory=True for faster GPU transfer</li>
                    <li><strong>Prefetch factor:</strong> Adjust prefetch_factor for optimal performance</li>
                </ul>

                <pre><code>train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=4,
    pin_memory=True,
    prefetch_factor=2,
    persistent_workers=True
)</code></pre>

                <h2>4. Gradient Checkpointing</h2>

                <p>
                    For very deep models, gradient checkpointing trades computation for memory by recomputing
                    intermediate
                    activations during the backward pass instead of storing them.
                </p>

                <img src="../images/gradient-checkpointing.png" alt="Gradient Checkpointing Illustration">

                <pre><code>import torch.utils.checkpoint as checkpoint

class CheckpointedModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.TransformerEncoderLayer(d_model=512, nhead=8)
            for _ in range(24)
        ])
    
    def forward(self, x):
        for layer in self.layers:
            x = checkpoint.checkpoint(layer, x)
        return x</code></pre>

                <h2>5. Distributed Training</h2>

                <p>
                    Distributed Data Parallel (DDP) is the recommended way to do multi-GPU training in PyTorch:
                </p>

                <pre><code>import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# Initialize process group
dist.init_process_group(backend='nccl')

# Create model and move to GPU
model = Model().cuda()
model = DDP(model)

# Use DistributedSampler
train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
train_loader = DataLoader(
    train_dataset, 
    sampler=train_sampler,
    batch_size=batch_size_per_gpu
)</code></pre>

                <h2>6. Profile and Optimize</h2>

                <p>
                    Always profile your code to identify bottlenecks. PyTorch provides excellent profiling tools:
                </p>

                <pre><code>from torch.profiler import profile, record_function, ProfilerActivity

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    profile_memory=True,
    with_stack=True
) as prof:
    for step, (data, target) in enumerate(train_loader):
        if step >= 10:  # Profile only 10 steps
            break
        with record_function("forward"):
            output = model(data)
        with record_function("backward"):
            loss = criterion(output, target)
            loss.backward()
        optimizer.step()

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))</code></pre>

                <h2>7. Memory-Efficient Optimizers</h2>

                <p>
                    Some optimizers like Adam store additional state that can consume significant memory. Consider using
                    memory-efficient variants:
                </p>

                <ul>
                    <li><strong>AdamW with 8-bit states:</strong> Using bitsandbytes library</li>
                    <li><strong>Adafactor:</strong> Reduces memory by factorizing second moments</li>
                    <li><strong>SGD with momentum:</strong> Simple but effective for many tasks</li>
                </ul>

                <h2>Best Practices Checklist</h2>

                <blockquote>
                    <p>
                        "Premature optimization is the root of all evil" - but knowing these techniques helps you scale
                        when needed.
                    </p>
                </blockquote>

                <ol>
                    <li>Start simple and profile to identify bottlenecks</li>
                    <li>Enable mixed precision training for compatible hardware</li>
                    <li>Optimize data loading pipeline</li>
                    <li>Use gradient accumulation for larger effective batch sizes</li>
                    <li>Consider distributed training for multi-GPU setups</li>
                    <li>Monitor GPU utilization and memory usage</li>
                    <li>Experiment with different optimizers and learning rate schedules</li>
                </ol>

                <h2>Conclusion</h2>

                <p>
                    Efficient training is about finding the right balance between speed, memory usage, and model
                    performance.
                    The techniques discussed here can significantly improve your training efficiency, allowing you to
                    iterate
                    faster and train larger models.
                </p>

                <p>
                    Remember that not all optimizations are suitable for every use case. Profile your specific workload
                    and
                    apply optimizations incrementally to ensure they provide real benefits for your particular scenario.
                </p>
            </div>

            <div class="blog-nav">
                <a href="understanding-transformers.html">← Previous: Understanding Transformers</a>
                <a href="mlops-best-practices.html">Next: MLOps Best Practices →</a>
            </div>
        </article>

        <footer>
            <p>&copy; 2026 Issa Memari. Built with ❤️ and vanilla HTML/CSS/JS.</p>
        </footer>
    </div>
</body>

</html>